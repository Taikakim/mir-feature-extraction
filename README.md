# MIR Feature Extraction Framework

!==! Very much a hot work-in-progress mess, but it alread kind of works for me and my RX 9070 XT, so I thought, it could be a good starting point for other people too. !==!
!==! None of the algos have been reviewed yet for correct implementation. I will leave that to the end since fixing implementation errors should not break anything, unlike the kind of total overhauls the code is still seeing through. !==!

**Comprehensive music feature extraction pipeline for conditioning Stable Audio Tools and similar audio generation models.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: TBD](https://img.shields.io/badge/license-TBD-lightgrey.svg)](LICENSE)

---

## Overview

This framework extracts **77 music information retrieval (MIR) features** from audio files, specifically designed as conditioning data for Stable Audio Tools training. It analyzes both full mixes and separated stems to capture comprehensive musical characteristics.

### Key Features

âœ… **77+ Numeric Features** extracted per track
âœ… **AI-Powered Descriptions** using Music Flamingo (8B params)
âœ… **Genre/Mood/Instrument Classification** (400 genres, 56 moods, 40 instruments)
âœ… **4-Stem Separation** using Demucs HT v4 (drums, bass, other, vocals)
âœ… **GPU Accelerated** processing (AMD ROCm / NVIDIA CUDA)
âœ… **Batch Processing** with automatic organization
âœ… **Safe JSON Updates** with feature merging
âœ… **Production Ready** - optimized for AMD RDNA4 (RX 9070 XT)

---

## Feature Categories

### Numeric Features (77+)

| Category | Features | Description |
|----------|----------|-------------|
| **Rhythm** | 29 | BPM, beats, syncopation, onset density, complexity (full + per-stem) |
| **Loudness** | 10 | LUFS/LRA for full mix + 4 stems (ITU-R BS.1770) |
| **Spectral** | 4 | Flatness, flux, skewness, kurtosis |
| **RMS Energy** | 4 | Bass, body, mid, air frequency bands (dB) |
| **Chroma** | 12 | 12-dimensional pitch class weights |
| **Harmonic** | 4 | Movement and variance for bass/other stems |
| **Timbral** | 8 | Audio Commons perceptual features |
| **Aesthetics** | 4 | Content enjoyment/usefulness, production complexity/quality |
| **Classification** | 2 | Danceability, atonality |

### AI Classification Features

| Category | Labels | Description |
|----------|--------|-------------|
| **Genre** | 400 | Discogs taxonomy (Blues, Electronic, Rock, Jazz, etc.) |
| **Mood/Theme** | 56 | MTG Jamendo (energetic, calm, dark, happy, epic, etc.) |
| **Instrument** | 40 | MTG Jamendo (guitar, drums, piano, synthesizer, etc.) |

### Natural Language Descriptions (Music Flamingo)

| Prompt Type | Description |
|-------------|-------------|
| **full** | Comprehensive description (genre, tempo, key, instruments, mood) |
| **technical** | Tempo, key, chords, dynamics, performance analysis |
| **genre_mood** | Genre classification + emotional character |
| **instrumentation** | Instruments and sounds present |
| **structure** | Arrangement and structure analysis |

**Total: 77+ numeric features + 496 classification labels + 5 AI descriptions**

---

## Quick Start

### 1. Clone and Setup

```bash
# Clone repository
git clone https://github.com/yourusername/mir.git
cd mir

# Create virtual environment
python3 -m venv mir
source mir/bin/activate  # Windows: mir\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup external repositories and apply patches
bash scripts/setup_external_repos.sh
# Or use Python script: python scripts/apply_patches.py
```

### 2. Organize Your Audio Files

```bash
# Convert flat audio files into organized folders
python src/preprocessing/file_organizer.py /path/to/audio/
```

**Before:**
```
/music/
â”œâ”€â”€ song1.mp3
â”œâ”€â”€ song2.flac
â””â”€â”€ song3.wav
```

**After:**
```
/music/
â”œâ”€â”€ Artist1 - Album1 - Song1/
â”‚   â””â”€â”€ full_mix.mp3
â”œâ”€â”€ Artist2 - Album2 - Song2/
â”‚   â””â”€â”€ full_mix.flac
â””â”€â”€ Artist3 - Album3 - Song3/
    â””â”€â”€ full_mix.wav
```

### 3. Separate Stems

```bash
# Use GPU for fast processing (~9-15x realtime)
python src/preprocessing/demucs_sep.py /path/to/audio/ --batch --device cuda

# Or process single track
python src/preprocessing/demucs_sep.py "/path/to/audio/Artist - Album - Track/"
```

**GPU Performance:**
- AMD (ROCm) / NVIDIA (CUDA): ~9-15x realtime (10min track in 40-60 seconds)
- CPU: ~0.1x realtime (10min track in 60+ minutes)

### 4. Extract Features

```bash
# Test single file with all features
python src/test_all_features.py "/path/to/Track Name/full_mix.flac"

# Or use --transformers flag for A/B testing with full Music Flamingo model
python src/test_all_features.py "/path/to/Track Name/full_mix.flac" --transformers

# Run modules individually for batch processing
python src/rhythm/rhythm_analysis.py /path/to/audio/ --batch
python src/preprocessing/loudness.py /path/to/audio/ --batch
python src/spectral/spectral_features.py /path/to/audio/ --batch
python src/harmonic/chroma_analysis.py /path/to/audio/ --batch
python src/harmonic/per_stem_harmonic.py /path/to/audio/ --batch
python src/timbral/audio_commons.py /path/to/audio/ --batch
python src/classification/essentia_features.py /path/to/audio/ --batch --gmi
python src/rhythm/per_stem_rhythm.py /path/to/audio/ --batch

# Music Flamingo AI descriptions (requires 16GB VRAM)
python src/classification/music_flamingo_transformers.py /path/to/audio/ --batch --flash-attention
```

**Processing Time:** ~35s per track (standard features), ~2.5min per track (with all 5 AI descriptions)

### 5. Create Training Crops (Optional)

```bash
# Sequential mode: exact sample length, no beat alignment
python src/tools/create_training_crops.py /path/to/audio/ --length 2097152 --sequential

# Beat-aligned with 50% overlap
python src/tools/create_training_crops.py /path/to/audio/ --length 2097152 --overlap

# Beat-aligned with div4 downbeats (measures divisible by 4)
python src/tools/create_training_crops.py /path/to/audio/ --length 2097152 --overlap --div4
```

**Crop Features:**
- Length in samples (default 2097152 = ~47.5s at 44.1kHz)
- Beat-aligned start/end with zero-crossing snap for click-free cuts
- Optional `--div4` ensures each crop contains downbeats divisible by 4
- 10ms fade-in/fade-out on all crops
- Metadata saved as `.json` sidecars with position, sample counts, downbeats

### 6. Access Results

Features saved to `.INFO` JSON files:

```json
{
  "bpm": 142.19,
  "bpm_is_defined": 1,
  "danceability": 0.987,
  "lufs": -12.73,
  "brightness": 58.64,
  "essentia_genre": {"Electronic---House": 0.85, "Electronic---Tech House": 0.72},
  "essentia_mood": {"energetic": 0.91, "groovy": 0.78, "party": 0.65},
  "essentia_instrument": {"synthesizer": 0.89, "drums": 0.95, "bass": 0.82},
  "music_flamingo_full": "This track is an energetic house music production...",
  "music_flamingo_technical": "The tempo is approximately 128 BPM...",
  ...
}
```

---

## Output Structure

```
Artist - Album - Track/
â”œâ”€â”€ full_mix.mp3                           # Original audio
â”œâ”€â”€ drums.mp3                              # Separated stems (MP3 @ 320kbps)
â”œâ”€â”€ bass.mp3
â”œâ”€â”€ other.mp3
â”œâ”€â”€ vocals.mp3
â”œâ”€â”€ Artist - Album - Track.INFO            # 77 features (JSON)
â”œâ”€â”€ Artist - Album - Track.BEATS_GRID      # Beat timestamps (JSON)
â””â”€â”€ separated/                             # Demucs working directory
    â””â”€â”€ htdemucs/
        â””â”€â”€ ...
```

---

## Documentation

- **[USER_MANUAL.md](USER_MANUAL.md)** - Comprehensive usage guide
- **[FEATURES_STATUS.md](FEATURES_STATUS.md)** - Complete feature implementation status
- **[EXTERNAL_PATCHES.md](EXTERNAL_PATCHES.md)** - External repository modifications
- **[project.log](project.log)** - Development history and decisions
- **[IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)** - Original implementation plan

---

## Project Structure

```
mir/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ core/                    # Core utilities (JSON, file handling, logging)
â”‚   â”œâ”€â”€ preprocessing/           # File organization, stem separation, loudness
â”‚   â”œâ”€â”€ rhythm/                  # Beat detection, BPM, syncopation, onsets
â”‚   â”œâ”€â”€ spectral/                # Spectral features and RMS energy
â”‚   â”œâ”€â”€ harmonic/                # Chroma, harmonic movement
â”‚   â”œâ”€â”€ timbral/                 # Audio Commons timbral features
â”‚   â””â”€â”€ classification/          # Essentia, Music Flamingo AI descriptions
â”œâ”€â”€ scripts/                      # Setup and utility scripts
â”‚   â”œâ”€â”€ setup_external_repos.sh  # Clone and patch external dependencies
â”‚   â””â”€â”€ apply_patches.py         # Apply librosa compatibility patches
â”œâ”€â”€ plans/                        # Implementation plan files
â”œâ”€â”€ repos/                        # External repositories (not tracked)
â”‚   â””â”€â”€ repos/
â”‚       â””â”€â”€ timbral_models/      # Audio Commons (cloned + patched)
â”œâ”€â”€ test_data/                    # Test audio (not tracked)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ USER_MANUAL.md               # Usage documentation
â”œâ”€â”€ FEATURES_STATUS.md           # Implementation status
â”œâ”€â”€ EXTERNAL_PATCHES.md          # External patches documentation
â””â”€â”€ project.log                  # Development log
```

---

## Requirements

### System Requirements

- **Python:** 3.10 or higher
- **GPU:** AMD (ROCm) or NVIDIA (CUDA) recommended for stem separation
- **Disk Space:** ~50MB per minute of audio (stems + features)
- **Memory:** 8GB+ RAM recommended

### Python Dependencies

Core libraries:
- **librosa** >= 0.11.0 - Audio analysis
- **soundfile** >= 0.12.0 - Audio I/O
- **demucs** >= 4.0.0 - Stem separation
- **pyloudnorm** >= 0.1.0 - Loudness measurement
- **essentia** >= 2.1b6 - High-level features
- **numpy**, **scipy**, **pandas** - Scientific computing

See `requirements.txt` for complete list.

### External Dependencies

- **timbral_models** (Audio Commons) - Cloned and patched automatically via setup script

---

## GPU Support

### AMD GPUs (ROCm)

```bash
# Use --device cuda (AMD GPUs appear as CUDA with ROCm)
python src/preprocessing/demucs_sep.py audio/ --batch --device cuda
```

**Performance:** ~9.4x realtime (10min track in ~64 seconds)

**Optimizations for AMD RDNA4:** Set these environment variables for best performance:

```bash
export PYTORCH_ALLOC_CONF=expandable_segments:True
export FLASH_ATTENTION_TRITON_AMD_ENABLE=TRUE
export PYTORCH_TUNABLEOP_ENABLED=1
export PYTORCH_TUNABLEOP_TUNING=0
export OMP_NUM_THREADS=8
```

### NVIDIA GPUs (CUDA)

```bash
# Use --device cuda
python src/preprocessing/demucs_sep.py audio/ --batch --device cuda
```

**Performance:** ~10-15x realtime

### CPU Only

```bash
# Use --device cpu (default)
python src/preprocessing/demucs_sep.py audio/ --batch
```

**Performance:** ~0.1x realtime (very slow)

---

## Known Issues & Solutions

### âœ… Audio Commons librosa API Errors (FIXED)

**Issue:** `onset_detect() takes 0 positional arguments but 2 positional arguments were given`

**Solution:** Automatically fixed by running `scripts/apply_patches.py` during setup. Patches documented in `EXTERNAL_PATCHES.md`.

### âœ… TorchCodec/FFmpeg Errors (FIXED)

**Issue:** Demucs fails to save output files with TorchCodec errors

**Solution:** Framework uses MP3 output @ 320kbps which bypasses TorchCodec compatibility issues. Quality sufficient for MIR analysis.

### âœ… AudioBox Aesthetics (FIXED)

**Status:** AudioBox aesthetics features now use actual Meta AudioBox model inference.

**Solution:** Implemented in `src/timbral/audiobox_aesthetics.py` using `audiobox_aesthetics` package from GitHub.

---

## Feature Implementation Status

âœ… **Implemented:** 77+ numeric features + AI classification

**Core Features:**
- âœ… All rhythm features (29)
- âœ… All loudness features (10)
- âœ… All spectral features (4)
- âœ… All RMS energy features (4)
- âœ… All chroma features (12)
- âœ… All harmonic features (4)
- âœ… All timbral features (8)
- âœ… All aesthetic features (4)
- âœ… All classification features (2)

**AI Features:**
- âœ… Genre classification (400 Discogs genres)
- âœ… Mood/theme classification (56 MTG Jamendo labels)
- âœ… Instrument classification (40 MTG Jamendo labels)
- âœ… Music Flamingo descriptions (5 prompt types)

**Missing:**
- âœ… Position feature (implemented via smart cropping system)

See [FEATURES_STATUS.md](FEATURES_STATUS.md) for complete details.

---

## Development Roadmap

### âœ… Phase 1: Core Feature Extraction (COMPLETE)
- File organization system
- Stem separation (Demucs)
- Rhythm analysis (BPM, beats, syncopation, onsets)
- Loudness analysis (LUFS/LRA per-stem)
- Spectral features (flatness, flux, skewness, kurtosis)
- RMS energy (4 frequency bands)
- Harmonic features (chroma, movement, variance)
- Timbral features (Audio Commons 8 features)
- Classification (danceability, atonality, aesthetics)

### âœ… Phase 2: Dataset Preparation (COMPLETE)
- âœ… Smart cropping system (`src/tools/create_training_crops.py`)
- âœ… Position feature calculation (via cropping metadata)
- âœ… AudioBox model inference (`src/timbral/audiobox_aesthetics.py`)
- Statistical analysis tool (planned)

### âœ… Phase 3: AI Classification (COMPLETE)
- Genre classification (400 Discogs genres via Essentia)
- Mood/theme classification (56 MTG Jamendo labels)
- Instrument classification (40 MTG Jamendo labels)
- Music Flamingo AI descriptions (5 prompt types)

### ðŸ“… Phase 4: Advanced Features (PLANNED)
- MIDI transcription (drums, bass, polyphonic)
- Kick/snare/cymbal per-drum analysis
- Auxiliary file outputs (.ONSETS_GRID, .CHROMA)

---

## Contributing

Contributions welcome! Areas of interest:
- Smart cropping algorithm implementation
- AudioBox aesthetics model integration
- MIDI transcription pipeline
- Additional feature extractors
- Documentation improvements

---

## Citation

If you use this framework in your research, please cite:

```
[Citation to be added]
```

Based on:
- [Stable Audio Tools](https://github.com/Stability-AI/stable-audio-tools) (Stability AI)
- [Essentia](https://essentia.upf.edu/) (Music Technology Group, UPF Barcelona)
- [Demucs](https://github.com/facebookresearch/demucs) (Meta Research)
- [Audio Commons Timbral Models](https://github.com/AudioCommons/timbral_models)
- [Music Flamingo](https://huggingface.co/nvidia/Music-Flamingo) (NVIDIA Research)

---

## License

[To be determined]

---

## Support

For issues, bugs, or feature requests:
1. Check [USER_MANUAL.md](USER_MANUAL.md) for usage help
2. Review [FEATURES_STATUS.md](FEATURES_STATUS.md) for implementation status
3. Consult [project.log](project.log) for known issues
4. Open an issue on GitHub

---

**Version:** 1.2
**Last Updated:** 2026-01-20
**Status:** Production Ready (Core + AI Features + Training Crops)
**Features:** 77+ numeric + 496 AI labels + 5 AI descriptions + smart cropping
